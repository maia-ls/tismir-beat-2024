# Other references

In this annex we provide a series of relevant references that were excluded from the main paper due to space limitations.

## 3.1 Rhythmic description
* Foote, J. and Uchihashi, S. (2001). The beat spectrum: A new approach to rhythm analysis. In Proceedings of the 2001 IEEE International Conference on Multimedia and Expo, pages 881–884, Tokyo, Japan.

* Tzanetakis, G. and Cook, P. (2002). Musical genre classification of audio signals. IEEE Transactions on Speech and Audio Processing, 10(5):293–302.

* Gouyon, F., Dixon, S., Pampalk, E., and Widmer, G. (2004). Evaluating rhythmic descriptors for musical genre classification. In Proceedings of the AES 25th International Conference, pages 196–204, London, UK.

* Paulus, J. and Klapuri, A. (2002). Measuring the similarity of rhythmic patterns. In Proceedings of the 3rd International Conference on Music Information Retrieval, pages 150–156, Paris, France.
    
* Peeters, G. (2005). Rhythm classification using spectral rhythm patterns. In Proceedings of the 6th International Conference on Music Information Retrieval, pages 644–647, London, UK.

* Holzapfel, A. and Stylianou, Y. (2008). Rhythmic similarity of music based on dynamic periodicity warping. In Proceedings of the 2008 IEEE International Conference on Acoustics, Speech and Signal Processing, pages 2217–2220, Las Vegas, USA.

* Marchand, U. and Peeters, G. (2014). The modulation scale spectrum and its application to rhythm content description. In Proceedings of the 17th Conference on Digital Audio Effects, pages 167–172, Erlangen, Germany.
        
* Marchand, U. and Peeters, G. (2016). Scale and shift invariant time/frequency representation using auditory statistics: Application to rhythm description. In Proceedings of the 2016 IEEE 26th International Workshop on Machine Learning for Signal Processing, pages 1–6, Vietri Sul Mare, Italy.

* Gruhne, M. and Dittmar, C. (2009). Improving rhythmic pattern features based on logarithmic preprocessing. In Proceedings of the AES 126th Convention, Munich, Germany.

* Jensen, J. H., Christensen, M. G., and Jensen, S. H. (2009). A tempo-insensitive representation of rhythmic patterns. In Proceedings of the 17th European Signal Processing Conference, pages 1509-1512, Glasgow, Scotland.

* Pampalk, E. (2006). Computational Models of Music Similarity and their Application in Music Information Retrieval. PhD thesis, Vienna University of Technology, Vienna, Austria.

* Pampalk, E., Rauber, A., and Merkl, D. (2002). Content-based organization and visualization of music archives. In Proceedings of the 10th ACM International Conference on Multimedia, pages 570-579, Juan-les-Pins, France.

* Esparza, T. M., Bello, J. P., and Humphrey, E. J. (2015). From genre classification to rhythm similarity: Computational and musicological insights. Journal of New Music Research, 44(1):39–57.

* Foroughmand, H. and Peeters, G. (2019). Deep-rhythm for global tempo estimation in music. In Proceedings of the 20th International Society for Music Information Retrieval Conference, pages 636–643, Delft, The Netherlands.
    
## 3.3 Active few-shot learning
### Few-shot learning
* Chou, S.-Y., Cheng, K.-H., Jang, J.-S. R., and Yang, Y.-H. (2019). Learning to match transient sound events using attentional similarity for few-shot sound recognition. In Proceedings 2019 IEEE International Conference on Acoustics, Speech and Signal Processing, pages 26–30, Brighton, UK.
            
* Cheng, K.-H., Chou, S.-Y., and Yang, Y.-H. (2019). Multi-label few-shot learning for sound event recognition. In Proceedings of the 2019 IEEE 21st International Workshop on Multimedia Signal Processing, Kuala Lumpur, Malaysia.
            
* Shi, B., Sun, M., Puvvada, K. C., Kao, C.-C., Matsoukas, S., and Wang, C. (2020). Few-shot acoustic event detection via meta learning. In Proceedings of the 2020 IEEE International Conference on Acoustics, Speech and Signal Processing, pages 76–80, Barcelona, Spain.
    
* Wang, Y., Salamon, J., Bryan, N. J., and Bello, J. P. (2020b). Few-shot sound event detection. In Proceedings of the 2020 IEEE International Conference on Acoustics, Speech and Signal Processing, pages 81–85, Barcelona, Spain.
    
* Wang, Y., Bryan, N. J., Cartwright, M., Pablo Bello, J., and Salamon, J. (2021a). Few-shot continual learning for audio classification. In Proceedings of the 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 321–325, Toronto, Canada.
            
* Wang, Y., Bryan, N. J., Salamon, J., Cartwright, M., and Bello, J. P. (2021b). Who calls the shots? rethinking few-shot learning for audio. In Proceedings of the 2021 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics, pages 36-40, New Paltz, USA.
    
* Garcia, H. F., Aguilar, A., Manilow, E., and Pardo, B. (2021). Leveraging hierarchical structures for few-shot musical instrument recognition. In Proceedings of the 22nd International Society for Music Information Retrieval Conference, pages 220–228, Online.

### Active learning
* Hakkani-Tür, D., Riccardi, G., and Gorin, A. (2002). Active learning for automatic speech recognition. In Proceedings of the 2002 IEEE International Conference on Acoustics, Speech, and Signal Processing, pages IV–3904–IV–3907.
    
* Yu, D., Varadarajan, B., Deng, L., and Acero, A. (2010). Active learning and semi-supervised learning for speech recognition: A unified framework using the global entropy reduction maximization criterion. Computer Speech \& Language, 24(3):433–444.
            
* Mandel, M. I., Poliner, G. E., and Ellis, D. P. (2006). Support vector machine active learning for music retrieval. Multimedia Systems, 12(1):3–13.
    
* Su, D. and Fung, P. (2012). Personalized music emotion classification via active learning. In Proceedings of the Second International ACM Workshop on Music Information Retrieval with User-Centered and Multimodal Strategies, page 57–62, Nara, Japan.
            
* Gómez-Cañón, J. S., Cano, E., Yang, Y.-H., Herrera, P., and Gomez, E. (2021). Let’s agree to disagree: Consensus entropy active learning for personalized music emotion recognition. In Proceedings of the 22nd International Society for Music Information Retrieval Conference, pages 237–245, Online.
    
* Wang, T.-J., Chen, G., and Herrera, P. (2009). Music retrieval based on a multi-samples selection strategy for support vector machine active learning. In Proceedings of the 24th ACM Symposium on Applied Computing, page 1750–1751.
            
* Han, W., Coutinho, E., Ruan, H., Li, H., Schuller, B., Yu, X., and Zhu, X. (2016). Semi-supervised active learning for sound classification in hybrid learning environments. PloS one, 11(9).
    
* Wang, Y., Mendez Mendez, A. E., Cartwright, M., and Bello, J. P. (2019). Active learning for efficient audio annotation and classification with a large amount of unlabeled data. In Proceedings of the 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 880–884, Brighton, UK.
            
* Li, W., Feng, X., and Xue, M. (2016). Reducing manual labeling in singing voice detection: An active learning approach. In Proceedings of the 2016 IEEE International Conference on Multimedia and Expo, Seattle, USA.
